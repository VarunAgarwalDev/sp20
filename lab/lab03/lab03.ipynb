{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Linear Regression and the Gauss-Markov theorem\n",
    "Welcome to DS102 lab!\n",
    "\n",
    "The goals of this lab are to get some practice with applying linear regression and to observe what happens when the Gauss-Markov theorem is applicable in practice.\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "##  Course Policies\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually.** If you do discuss the assignments with others please include their names in the cell below.\n",
    "\n",
    "**Submission:** to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "**This assignment should be completed and submitted before Thursday February 13, 2020 at 11:59 PM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborators\n",
    "Write the names of your collaborators in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Let's begin by importing the libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Part 1: Ordinary Least Squares estimator\n",
    "In the first part of this lab, we will apply the ordinary least squares (OLS) estimator in the context of linear regression. The objective is to use linear regression to understand how the number of people waiting in line at a boba tea shop and the number of employees working at the shop correlates with the amount of time you have to spend at the boba tea shop before getting your tea. \n",
=======
    "# Part 1: Ordinary Least Squares estimator with zero-mean errors\n",
    "In the first part of this lab, we will apply the ordinary least squares (OLS) estimator in the context of linear regression. The objective is to use linear regression to understand how the number of hours a student spends per week on Data 102 affects the student's grade. \n",
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
    "\n",
    "## Ground truth model\n",
    "Let $y$ be the amount of time it takes to get your boba (in minutes), and let $x = [x_0, x_1]$ be a feature vector where $x_0$ is the number of people waiting in line to get boba, and $x_1$ is the number of employees behind the counter. For this lab, we provide the ground truth that $y = \\langle \\beta, x \\rangle$ for some true $\\beta = [\\beta_0, \\beta_1]$. Specifically, we set the true $\\beta$ values to be $\\beta_0 = 5$, and $\\beta_1 = -2$. In words, this means that for each additional person waiting in line, you'll have to wait $5$ more minutes, but for each employee behind the counter, you'll have to wait $2$ fewer minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## 1a) Simulate dataset\n",
    "Here we simulate the dataset described above. We draw $n$ samples where $x_0^{(1)},...,x_0^{(n)}$ are drawn from a uniform distribution between 0 and 30 people, and $x_1^{(1)},...,x_1^{(n)}$ are drawn from a uniform distribution between 0 and 5 people.\n",
    "\n",
    "Let $X$ be an $n \\times 2$ matrix where the $i$th row of $X$ is a data point $x^{(i)} = [x_0^{(i)}, x_1^{(i)}]$. Let $\\mathbf{y}$ be an $n$ dimensional vector where the $i$th entry is $y^{(i)} = \\langle \\beta, x^{(i)}\\rangle$.\n",
    "\n",
    "In this section, you will first calculate the ground truth $\\mathbf{y}$ from some randomly sampled $X$. Specifically, you will implement $\\mathbf{y} = X\\beta$, where $X\\beta$ is matrix multiplication between $X$ and $\\beta$."
=======
    "## Simulate dataset\n",
    "Below we simulate the dataset described above. We set the true $\\beta$ to be $5.0$. We draw $n$ samples where $x^{(1)},...,x^{(n)}$ are drawn from a uniform distribution between $0$ and $20$ hours per week, and $\\epsilon^{(1)},...,\\epsilon^{(n)}$ are normally distributed with mean $0$. The ground truth labels are $y^{(i)} = \\beta x^{(i)} + \\epsilon^{(i)}$."
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: simulate the data by calculating the ground truth y matrix.\n",
    "TRUE_BETA=np.array([5.0, -2.0])\n",
    "\n",
    "def simulate_data_no_error(n):\n",
    "    # Sample Xs.\n",
    "    x0 = np.random.choice(30, n)\n",
    "    x1 = np.random.choice(5, n)\n",
    "    X = np.array([x0, x1]).T\n",
    "    y = # TODO: Calculate ground truth ys. Hint: use np.matmul to do matrix multiplication between X and TRUE_BETA.\n",
    "    return X, y\n",
    "\n",
    "X, y = simulate_data_no_error(100)\n",
    "\n",
    "def plot_data(input_X, input_y, predictions = None):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    if predictions is not None:\n",
    "        ax.plot_trisurf(input_X.T[1], input_X.T[0], predictions, cmap='viridis', edgecolor='none')\n",
    "    ax.scatter3D(input_X.T[1], input_X.T[0], input_y, c=input_y, cmap='Reds', label=\"data\");\n",
    "    ax.set_xlabel(\"Number of employees\", fontsize=14)\n",
    "    ax.set_ylabel(\"Number of people in line\", fontsize=14)\n",
    "    ax.set_zlabel(\"Minutes spent waiting for boba\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Calculate the Ordinary Least Squares (OLS) estimator\n",
    "First we are interested in finding the best approximation $y^{(i)} \\approx \\langle x^{(i)},  \\hat{\\beta} \\rangle$, where $\\hat{\\beta}_0 = [\\hat{\\beta}_0,\\hat{\\beta}_1] $, and $\\hat{\\beta}_0$ is our estimate of how many additional minutes each person in line adds to our wait time, and $\\hat{\\beta}_1$ is our estimate of how many minutes each employee takes away from our wait time. We want to find the constant $\\hat{\\beta}$ that minimizes $\\sum_{i=1}^n (y^{(i)} - \\langle x^{(i)}, \\hat{\\beta}\\rangle )^2$. This estimator $\\hat{\\beta}$ is known as the **Ordinary Least Squares (OLS)** estimator.\n",
    "\n",
<<<<<<< HEAD
    "Specificaly, given a dataset of $n$ samples with $d$ features, let $X$ be an $n \\times d$ matrix where each row is a feature vector $x^{(i)}$, and let $\\mathbf{y}$ be an $n$ dimensional vector where each entry is a label $y^{(i)}$. Recall from class that the OLS estimator that minimizes $\\sum_{i=1}^n (y^{(i)} - \\langle x^{(i)}, \\hat{\\beta}\\rangle)^2$ is:\n",
=======
    "Specificaly, given a dataset of $n$ samples with $d$ features, let $X$ be an $n \\times d$ matrix where each row is a feature vector $x^{(i)} $ in $\\mathbb{R}^d$, and let $y$ be a vector in $\\mathbb{R}^n$ where each entry is a label $y^{(i)}$. Recall from class that the OLS estimator that minimizes $\\sum_{i=1}^n (y^{(i)} - (\\hat{\\beta})^Tx^{(i)})^2$ is:\n",
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_beta_hat(X, y):\n",
    "    \"\"\"Calculates the OLS estimator beta_hat.\n",
    "    \n",
    "    Args:\n",
    "      X: numpy array with n rows and 2 columns, where each row corresponds with a feature vector [x0, x1].\n",
    "      y: numpy array with n entries, where each entry corresponds with a label value for a given sample.\n",
    "    \n",
    "    Returns:\n",
    "      beta_hat: numpy array with 2 entries, where the entries are [beta_hat_0, beta_hat_1].\n",
    "    \"\"\"\n",
    "    # TODO: using the formula above, calculate the OLS estimator beta_hat. \n",
    "    # Hint: use np.linalg.inv to take a matrix inverse. Use np.dot to take a dot product, \n",
    "    # or np.matmul for matrix multiplication.\n",
    "    beta_hat = # TODO: fill this in.\n",
    "    return beta_hat\n",
    "\n",
    "beta_hat = calculate_beta_hat(X, y)\n",
    "print(\"The OLS estimator beta_hat is:\", (beta_hat))\n",
    "\n",
    "# Test that the estimator beta_hat basically equals TRUE_BETA\n",
    "assert(beta_hat.astype(int).all() == TRUE_BETA.astype(int).all())\n",
    "print(\"Test passed! :) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OLS predictions\n",
    "Given this $\\hat{\\beta}$, we compute the predictions for the points $x^{(i)}$ we have in the data set. For each data point $x^{(i)}$, the prediction we compute is $\\langle x^{(i)}, \\hat{\\beta} \\rangle$. This means that if $X$ is a matrix where the $i$th row is $x^{(i)}$, then the vector of all $n$ predictions should be $X \\hat{\\beta}$ (this is a matrix multiplication between $X$ and $\\hat{\\beta}$).\n",
    "\n",
    "In this part, we will compute these predictions and then plot the predictions to see how well they fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# TODO: calculate the predictions from the beta_hat estimated above. Hint: use np.matmul again.\n",
    "def compute_OLS_predictions(X, beta_hat):\n",
    "    \"\"\"Computes OLS predictions given data X and OLS estimator beta_hat.\n",
    "    \n",
    "    Args: \n",
    "      X: numpy array with n rows and 2 columns, where each row represents a feature vector [x0, x1].\n",
    "      beta_hat: numpy array with 2 entries representing the OLS estimator.\n",
    "      \n",
    "    Returns:\n",
    "      y: numpy array with n rows and 1 column, where each row represents an output <x^{(i)}, beta_hat>.\n",
    "    \"\"\"\n",
    "    # TODO: calculate the predictions from the beta_hat estimated above. \n",
    "    # Hint: this is similar to 1a). use np.matmul again.\n",
    "    predictions = # TODO: fill this in.\n",
    "    return predictions\n",
    "\n",
    "predictions = compute_OLS_predictions(X, beta_hat)\n",
=======
    "# TODO: calculate the predictions from the beta_hat estimated above.\n",
    "predictions = # TODO\n",
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
    "plot_data(X, y, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Part 2: OLS estimation with errors in the label $y$\n",
    "In this part, we will observe what happens to the OLS estimator when there are errors in the label $y$."
=======
    "## 1b) Gauss-Markov Theorem:\n",
    "### Question: If we use the OLS estimator $\\hat{\\beta}$ to estimate the true $\\beta$ from the dataset we simulated above, does the Gauss-Markov theorem apply?\n",
    "\n",
    "TODO: fill in your answer.\n",
    "\n",
    "Answer: yes, the errors $\\epsilon$ have mean 0 conditioned on each $x^{(i)}$."
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## 2a) Simulate dataset\n",
    "We will simulate a dataset with error in the labels by letting the observed $y^{(i)} = \\langle x^{(i)}, \\beta \\rangle + \\epsilon^{(i)}$, where $\\epsilon^{(i)}$ is random error drawn from a normal distribution with mean $0$ and variance $10$. This error simulates the idea that maybe the number of minutes that you end up waiting for boba varies from day to day by approximately plus or minus $10$ minutes."
=======
    "If the Gauss-Markov theorem applies, then averaged over the randomness from the random draws of data, the mean of the OLS estimator $\\hat{\\beta}$ should equal the true $\\beta$. Below, we calculate the mean and standard deviation of $\\hat{\\beta}$ by conducting multiple trials with sample size $n=100$, and averaging over the OLS estimator $\\hat{\\beta}$ found in each trial. "
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# TODO: simulate a dataset with errors in the label y.\n",
    "def simulate_data_label_error(n):\n",
    "    # Sample Xs.\n",
    "    x0 = np.random.choice(30, n)\n",
    "    x1 = np.random.choice(5, n)\n",
    "    X = np.array([x0, x1]).T\n",
    "    # Sample epsilon errors. epsilons is an n x 1 matrix where each entry is a given epsilon^{(i)}.\n",
    "    epsilons = np.random.normal(0,10,n)\n",
    "    \n",
    "    # TODO: add epsilon errors to the label y. Hint: use np.add.\n",
    "    y = # TODO: fill this in.\n",
    "    return X, y\n",
    "\n",
    "X_label_error, y_label_error = simulate_data_label_error(100)\n",
=======
    "def OLS_estimate_mean_and_std(num_trials=50, n=100):\n",
    "    \"\"\"Calculates the mean and standard deviation of the OLS estimate `beta_hat` over `num_trials` random trials.\n",
    "    \n",
    "    Hint: For each trial in `num_trials`, you should draw `n` samples of data, and calculate `beta_hat` \n",
    "    from those `n` samples. After that, you should end up with `num_trials` different `beta_hat`s. \n",
    "    Compute the mean and standard deviation of the resulting `beta_hat`s.\n",
    "\n",
    "    Args: num_trials: Number of trials. Calculate a different `beta_hat` for each trial using `n` samples,\n",
    "      and find the mean and standard deviation of all of the `num_trials` different `beta_hat`s you calculated.\n",
    "    \n",
    "    Returns: `beta_hat_mean`, `beta_hat_std`. `beta_hat_mean` is the mean of `beta_hat` over `num_trials` \n",
    "      trials, and `beta_hat_std` is the standard deviation of `beta_hat` over `num_trials` trials. \n",
    "      Hint: try the numpy functions `np.mean` and `np.std`. Google these functions if you get stuck.\n",
    "    \n",
    "    \"\"\"\n",
    "    beta_hats = []\n",
    "    for _ in range(num_trials):\n",
    "        \n",
    "        beta_hat = # TODO: calculate the mean and standard deviation of `beta_hat` over `num_trials` random trials.\n",
    "        beta_hats.append(beta_hat)\n",
    "    beta_hats = np.array(beta_hats)\n",
    "    return np.mean(beta_hats), np.std(beta_hats)\n",
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
    "\n",
    "plot_data(X_label_error, y_label_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## 2b) Calculate the Ordinary Least Squares (OLS) estimator\n",
    "We will calculate the OLS estimator from this new simulated dataset that includes the $\\epsilon^{(i)}$ errors."
=======
    "If the Gauss-Markov theorem applies, then as the sample size $n$ grows, the OLS estimator $\\hat{\\beta}$ should approach the true $\\beta$. Below, we calculate $\\hat{\\beta}$ for different sample sizes $n$ ranging from small sample sizes (only $n=5$) to large sample sizes ($n = 1028$). "
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# No TODOs here, just run this cell.\n",
    "# We are reusing the function you wrote earlier to calculate beta_hat on this new dataset.\n",
    "beta_hat = calculate_beta_hat(X_label_error, y_label_error)\n",
    "print(\"The OLS estimator beta_hat is:\", (beta_hat))"
=======
    "def OLS_estimates_different_sample_sizes(sample_sizes):\n",
    "    \"\"\"Computes OLS estimate beta_hat for each sample size in `sample_sizes`.\n",
    "    \n",
    "    Args: sample_sizes: list of integers, each representing a sample size n.\n",
    "    \n",
    "    Returns: beta_hats: list of `beta_hats`, same length as `sample_sizes`.\n",
    "      Each entry in `beta_hats` corresponds to the OLS estimate beta_hat for a \n",
    "      sample size in `sample_sizes`.\n",
    "    \"\"\"\n",
    "    beta_hats=[]\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        \n",
    "        beta_hat =  # TODO: calculate beta_hat for each sample size.\n",
    "        beta_hats.append(beta_hat)\n",
    "        \n",
    "    return beta_hats"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Compute OLS predictions\n",
    "Given this $\\hat{\\beta}$, we compute the predictions for the points $x^{(i)}$ we have in the data set. \n",
    "Here, we will compute these predictions, and then plot the predictions to see how well they fit the data with error in the labels."
=======
    "# Plot resulting beta_hats for each sample size.\n",
    "# No TODOs here, just run this cell.\n",
    "sample_sizes = [4+2**i for i in range(11)]\n",
    "print(\"Getting beta_hat estimates for sample sizes:\", sample_sizes)\n",
    "beta_hats = OLS_estimates_different_sample_sizes(sample_sizes)\n",
    "plt.plot(sample_sizes, beta_hats, label='$\\hat{\\\\beta}$ estimate')\n",
    "plt.plot(sample_sizes, TRUE_BETA*np.ones_like(sample_sizes), label='true $\\\\beta$', color='black')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sample size $n$\", fontsize=14)\n",
    "plt.ylabel(\"$\\hat{\\\\beta}$ estimate\", fontsize=14)\n",
    "plt.show()"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# No TODOs here, just run this cell. \n",
    "# We are reusing the function you wrote earlier to compute OLS predictions on this new dataset.\n",
    "predictions = compute_OLS_predictions(X_label_error, beta_hat)\n",
    "plot_data(X_label_error, y_label_error, predictions=predictions)"
=======
    "### Question: Does the OLS estimate $\\hat{\\beta}$ appear to satisfy the Gauss-Markov theorem based on its behavior as $n$ grows? \n",
    "\n",
    "TODO: fill in your answer\n"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## 2c) Observe how the OLS estimator $\\hat{\\beta}$ varies when the $\\epsilon$ errors are included.\n",
    "Since the simulated dataset now has some random errors, we will observe that the OLS estimator $\\hat{\\beta}$ will not always be the same every time you generate new data. In this part, we will generate new data 5 times, and observe what the OLS estimator looks like for each of those 5 different draws."
=======
    "# Part 2: Biased $\\epsilon$ errors\n",
    "Previously, the $\\epsilon$ errors all had mean 0. Therefore, the Gauss-Markov theorem applied, and the OLS estimator $\\hat{\\beta}$ worked well on average: it approached the true $\\beta$ as the number of samples increased. In this part, we explore what happens when the $\\epsilon$ errors do NOT have mean 0.\n",
    "\n",
    "\n",
    "Suppose students who spent between 4 and 12 hours (including 12 but not including 4) on the class tended to over-report the number of hours per week they spent on the class. Then the grades we observe for students who reported between 4 and 12 hours will be lower than the true average grades of students who *actually* spent between 4 and 12 hours per week on the class. To simulate this, for any examples where students reported between 4 to 12 hours per week on the class, we generate the data with an $\\epsilon$ error that has a negative mean."
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# No TODOs here, just run this cell and observe how the OLS estimator beta_hat varies for each \n",
    "# different random draw of data.\n",
    "num_random_draws = 5\n",
    "num_samples = 100\n",
    "for i in range(num_random_draws):\n",
    "    X_label_error, y_label_error = simulate_data_label_error(num_samples)\n",
    "    beta_hat = calculate_beta_hat(X_label_error, y_label_error)\n",
    "    print(\"The OLS estimator beta_hat for random draw %d is:\" % i, (beta_hat))"
=======
    "# No TODOs here, just run this cell and observe the data with biased errors compared the previous data with\n",
    "# zero-mean errors.\n",
    "def simulate_data_biased(n):\n",
    "    xs = np.random.uniform(0,20,n)\n",
    "    epsilons = []\n",
    "    epsilon_mean_less_than_12 = -20\n",
    "    for x in X: \n",
    "        # We assume students who spent fewer than 4 hours on the class were actually honest.\n",
    "        if x <= 12 and x > 4:\n",
    "            epsilon = np.random.normal(epsilon_mean_less_than_12,10)\n",
    "            epsilons.append(epsilon)\n",
    "        else: \n",
    "            epsilon = np.random.normal(0,10)\n",
    "            epsilons.append(epsilon)\n",
    "    y = TRUE_BETA * X + np.array(epsilons)\n",
    "    return X, y\n",
    "\n",
    "def plot_data_biased_and_unbiased(X_unbiased, y_unbiased, X_biased, y_biased, predictions = None):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.scatter(X_unbiased, y_unbiased, label=\"Data with zero mean $\\epsilon$ errors\")\n",
    "    plt.scatter(X_biased, y_biased, label=\"Data with biased $\\epsilon$ errors\")\n",
    "    if predictions is not None:\n",
    "        plt.plot(input_xs, predictions, c='blue', linewidth=2)\n",
    "    plt.xlabel(\"Number of hours per week spent on Data 102\", fontsize=14)\n",
    "    plt.ylabel(\"Grade\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "n = 100\n",
    "X_biased, y_biased = simulate_data_biased(n)\n",
    "X_unbiased, y_unbiased = simulate_data_unbiased(n)\n",
    "plot_data_biased_and_unbiased(X_unbiased, y_unbiased, X_biased, y_biased)"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: For the five random draws above, does the OLS estimator $\\hat{\\beta}$ generally look like it's close to the true $\\beta$?\n",
    "\n",
    "TODO: fill in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Part 3: OLS estimation with errors in the features $x$\n",
    "In this part, we will observe what happens to the OLS estimator when there are errors in the observed feature vector $x$. Specifically, we will see what happens when we observe noise in the first feature $x_0$, and not the second feature."
=======
    "# Calculate `beta_hat` from the biased data.\n",
    "beta_hat = calculate_beta_hat(\"\"\"TODO\"\"\", \"\"\"TODO\"\"\") # TODO: fill this in \n",
    "print(\"The OLS estimator beta_hat is: %.5f\" % (beta_hat))\n",
    "print(\"The linear model is: Y = {:.5}X\".format(beta_hat))"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a) Simulate dataset\n",
    "We will simulate a dataset with error in the first feature $x_0$. Let $y^{(i)} = \\langle x^{(i)}, \\beta \\rangle$ as before, but now suppose instead of observing $x^{(i)}$, we observe $\\tilde{x}^{(i)}$ where $\\tilde{x}^{(i)} = [x^{(i)}_0 + \\epsilon^{(i)}, x^{(i)}_1]$. Here, $\\epsilon^{(i)}$ is a random error drawn from a normal distribution with mean $0$ and standard deviation $3$. This error simulates the idea that maybe you weren't super precise about counting exactly how many people were in line, and might have been off by plus or minus 3 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: simulate a dataset with errors in the first feature x0.\n",
    "def simulate_data_feature_error(n):\n",
    "    # Sample Xs as before.\n",
    "    x0 = np.random.choice(30, n)\n",
    "    x1 = np.random.choice(5, n)\n",
    "    X = np.array([x0, x1]).T\n",
    "    # Generate ys as before in part 1a).\n",
    "    y = np.matmul(X, TRUE_BETA)\n",
    "    # Sample epsilon errors. epsilons is an n dimensional vector where each entry is a given epsilon^{(i)}.\n",
    "    epsilons = np.random.normal(0,3,n)\n",
    "    \n",
    "    # TODO: add epsilon errors to the feature x0. Hint: use np.add again.\n",
    "    x0_tilde = # TODO: fill this in.\n",
    "    X_tilde = np.array([x0_tilde, x1]).T\n",
    "    return X_tilde, y\n",
    "\n",
<<<<<<< HEAD
    "X_feature_error, y_feature_error = simulate_data_feature_error(100)\n",
    "\n",
    "plot_data(X_feature_error, y_feature_error)"
=======
    "TODO: fill in your answer.\n"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) Calculate the Ordinary Least Squares (OLS) estimator\n",
    "We will calculate the OLS estimator from this new simulated dataset that includes the $\\epsilon^{(i)}$ errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# No TODOs here, just run this cell.\n",
    "# We are reusing the function you wrote earlier to calculate beta_hat on this new dataset.\n",
    "beta_hat = calculate_beta_hat(X_feature_error, y_feature_error)\n",
    "print(\"The OLS estimator beta_hat is:\", (beta_hat))"
=======
    "def OLS_estimate_mean_and_std_biased(num_trials=50, n=100):\n",
    "    \"\"\"Calculates the mean and standard deviation of the OLS estimate `beta_hat` over `num_trials` random trials\n",
    "    using the biased simulated data.\n",
    "    \n",
    "    Args: num_trials: Number of trials over which to average `beta_hat`. \n",
    "    \n",
    "    Returns: beta_hat_mean, beta_hat_std.\n",
    "    \"\"\"\n",
    "    beta_hats = []\n",
    "    for _ in range(num_trials):\n",
    "\n",
    "        beta_hat = # TODO: calculate the mean and standard deviation of `beta_hat` over `num_trials` random trials.\n",
    "        # This should be the same as your previous function, but using the biased simulated data.\n",
    "        beta_hats.append(beta_hat)\n",
    "    beta_hats = np.array(beta_hats)\n",
    "    return np.mean(beta_hats), np.std(beta_hats)\n",
    "\n",
    "mean, std = OLS_estimate_mean_and_std_biased()\n",
    "print(\"Mean of OLS estimate beta_hat after 50 random trials: %.5f\" % (mean))\n",
    "print(\"Standard deviation of OLS estimate beta_hat after 50 random trials: %.5f\" % (std))"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OLS predictions\n",
    "Given this $\\hat{\\beta}$, we compute the predictions for the points $\\tilde{x}^{(i)}$ we observe in the data set. \n",
    "Here, we will compute these predictions, and then plot the predictions to see how well they fit the data with error in the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell. \n",
    "# We are reusing the function you wrote earlier to compute OLS predictions on this new dataset.\n",
    "predictions = compute_OLS_predictions(X_feature_error, beta_hat)\n",
    "plot_data(X_feature_error, y_feature_error, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## 3c) Observe how the OLS estimator $\\hat{\\beta}$ varies when the $\\epsilon$ errors are included.\n",
    "Since the simulated dataset now has some random errors, we will observe that the OLS estimator $\\hat{\\beta}$ will not always be the same every time you generate new data. In this part, we will generate new data 5 times, and observe what the OLS estimator looks like for each of those 5 different draws."
=======
    "def OLS_estimates_different_sample_sizes_biased(sample_sizes):\n",
    "    \"\"\"Computes OLS estimate `beta_hat` for each sample size in `sample_sizes` used the biased simulated data.\n",
    "    \n",
    "    Args: sample_sizes: list of integers, each representing a sample size n.\n",
    "    \n",
    "    Returns: beta_hats: list of beta_hats, same length as `sample_sizes`.\n",
    "      Each entry in `beta_hats` corresponds to the OLS estimate `beta_hat` for a \n",
    "      sample size in `sample_sizes`.\n",
    "    \"\"\"\n",
    "    beta_hats=[]\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        \n",
    "        beta_hat =  # TODO: calculate beta_hat for each sample size.\n",
    "        # This should be the same as your previous function, but using the biased simulated data.\n",
    "        beta_hats.append(beta_hat)\n",
    "        \n",
    "    return beta_hats"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# No TODOs here, just run this cell and observe how the OLS estimator beta_hat varies for each \n",
    "# different random draw of data.\n",
    "num_random_draws = 5\n",
    "num_samples = 100\n",
    "for i in range(num_random_draws):\n",
    "    X_feature_error, y_feature_error = simulate_data_feature_error(num_samples)\n",
    "    beta_hat = calculate_beta_hat(X_feature_error, y_feature_error)\n",
    "    print(\"The OLS estimator beta_hat for random draw %d is:\" % i, (beta_hat))"
=======
    "# Plot resulting beta_hats for each sample size.\n",
    "# No TODOs here, just run this cell.\n",
    "print(\"Getting beta_hat estimates for sample sizes:\", sample_sizes)\n",
    "sample_sizes = [4+2**i for i in range(11)]\n",
    "beta_hats = OLS_estimates_different_sample_sizes_biased(sample_sizes)\n",
    "plt.plot(sample_sizes, beta_hats, label='$\\hat{\\\\beta}$ estimate')\n",
    "plt.plot(sample_sizes, TRUE_BETA*np.ones_like(sample_sizes), label='true $\\\\beta$', color='black')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sample size n\", fontsize=14)\n",
    "plt.ylabel(\"$\\hat{\\\\beta}$ estimate\", fontsize=14)\n",
    "plt.show()"
>>>>>>> 72cb3faa65a6330966cc1c08e7c3ac4effd6390a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How do the five random OLS estimators $\\hat{\\beta}$ in part 3c) compare to the five random OLS estimators $\\hat{\\beta}$ in part 2c)? Does one set of five tend to more closely revolve around the true $\\beta$ on average than the other?\n",
    "\n",
    "TODO: fill in your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this all relates to Gauss-Markov:\n",
    "In lecture, we showed that when we only observe error in the labels (i.e. $y = \\langle x, \\beta \\rangle + \\epsilon$), and $\\epsilon$ has zero-mean, the Gauss-Markov theorem applies, and the error shouldn't mess up our OLS regression estimate. On the other hand, noise in the covariates (aka the features) *can* mess up our OLS regression estimates. Specifically, noise in our covarites causes us to do implicit regularization via ridge regression, which formalizes the intuition that adding noise will lead to shrinking the coefficients towards zero. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
